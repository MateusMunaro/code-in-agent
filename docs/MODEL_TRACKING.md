# Model Selection & Tracking System

## ğŸ“Š Overview

Sistema completo de controle, monitoramento e rastreamento do uso de modelos LLM durante a anÃ¡lise de cÃ³digo.

## âœ¨ Features Implementadas

### 1. **ModelUsageStats** â€” Rastreamento Detalhado

Cada modelo agora rastreia:

- **Invocations**: NÃºmero total de chamadas
- **Successes**: Chamadas bem-sucedidas
- **Failures**: Chamadas que falharam
- **Success Rate**: Taxa de sucesso (%)
- **Total Tokens**: Tokens consumidos (estimado)
- **Avg Latency**: LatÃªncia mÃ©dia em milissegundos
- **Last Used**: Timestamp da Ãºltima invocaÃ§Ã£o

```python
from agent.src.llm import MultiModelChat

chat = MultiModelChat()
# ... uso normal ...

# Ver estatÃ­sticas
stats = chat.get_usage_stats()
chat.print_usage_summary()
```

**Output exemplo**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Model Usage Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Session started: 2026-02-16 14:30:00
Total models used: 2

ğŸ”¹ gemini-2.5-flash (google)
   Invocations: 15 | Success: 14 | Failed: 1
   Success Rate: 93.3%
   Total Tokens: 42,156
   Avg Latency: 1,234ms
   Last Used: 14:35:22

ğŸ”¹ gemini-2.0-flash (google)
   Invocations: 1 | Success: 1 | Failed: 0
   Success Rate: 100.0%
   Total Tokens: 2,891
   Avg Latency: 890ms
   Last Used: 14:32:15

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

### 2. **Health Checks** â€” ValidaÃ§Ã£o PrÃ©-voo

Cada provider agora implementa `health_check()`:

- **OpenAI**: Verifica presenÃ§a de API key + instanciaÃ§Ã£o do cliente
- **Anthropic**: Verifica presenÃ§a de API key + instanciaÃ§Ã£o do cliente  
- **Google AI**: Verifica presenÃ§a de API key + instanciaÃ§Ã£o do cliente
- **Ollama**: Pinga o servidor local e lista modelos disponÃ­veis

```python
from agent.src.llm import check_providers_health

health_status = check_providers_health()
# SaÃ­da:
# [Provider Health] âœ… google: Google AI provider configured
# [Provider Health] âŒ openai: OpenAI API key not configured
# [Provider Health] âŒ anthropic: Anthropic API key not configured
# [Provider Health] âŒ ollama: Cannot connect to Ollama at http://localhost:11434
```

---

### 3. **Smart Model Selection** â€” PriorizaÃ§Ã£o Inteligente

O sistema agora:

1. Verifica quais providers estÃ£o configurados
2. Seleciona o primeiro modelo disponÃ­vel na lista de prioridade
3. Loga a decisÃ£o de seleÃ§Ã£o
4. Valida o modelo antes de usar

```python
from agent.src.llm import get_default_model, validate_model_before_use

# SeleÃ§Ã£o automÃ¡tica
model = get_default_model()
# [Model Selection] Available models: ['gemini-2.5-flash', 'gemini-2.0-flash']
# [Model Selection] âœ… Selected: gemini-2.5-flash

# ValidaÃ§Ã£o
is_valid, msg = validate_model_before_use(model)
if not is_valid:
    print(f"âš ï¸ {msg}")
```

**Priority list** (configurÃ¡vel em `provider.py`):
1. `gemini-2.5-flash` â€” RÃ¡pido e capaz
2. `gemini-2.0-flash` â€” Fallback rÃ¡pido
3. `gemini-2.5-pro` â€” Fallback Pro
4. `gemini-3-flash` â€” Ãšltima geraÃ§Ã£o
5. `gemini-3-pro` â€” Mais poderoso

---

### 4. **Auto-Retry with Fallback** â€” ResiliÃªncia AutomÃ¡tica

Se uma chamada falhar, o sistema automaticamente:

1. Loga a falha
2. Registra nas estatÃ­sticas
3. Tenta novamente com `gemini-2.0-flash` (modelo de fallback)
4. SÃ³ lanÃ§a exceÃ§Ã£o se o fallback tambÃ©m falhar

```python
chat = MultiModelChat("gemini-2.5-pro")

# Se gemini-2.5-pro falhar, automaticamente tenta gemini-2.0-flash
response = await chat.ainvoke(messages, retry_on_failure=True)

# Logs gerados:
# [MultiModelChat] ğŸš€ Invoking: gemini-2.5-pro
# [MultiModelChat] âŒ Failed: gemini-2.5-pro - quota exceeded
# [MultiModelChat] ğŸ”„ Retrying with fallback: gemini-2.0-flash
# [MultiModelChat] âœ… Success: gemini-2.0-flash (890ms, ~2,541 tokens)
```

---

### 5. **Task Context** â€” Rastreamento por Etapa

Cada node do LangGraph agora declara seu contexto:

```python
class PlanningNode(BaseNode):
    async def __call__(self, state):
        self.chat.set_task_context("Planning")  # <-- Declara o contexto
        # ... lÃ³gica do node ...
```

**Logs gerados**:
```
[MultiModelChat] ğŸ“‹ Task: Planning
[MultiModelChat] ğŸš€ Invoking: gemini-2.5-flash
[MultiModelChat] [Planning] âœ… Success: gemini-2.5-flash (1,234ms, ~3,456 tokens)
```

Agora vocÃª sabe **exatamente** qual etapa do pipeline estÃ¡ usando qual modelo!

---

### 6. **Detailed Logging** â€” Visibilidade Total

Todos os eventos sÃ£o logados:

- âœ… **InicializaÃ§Ã£o**: Qual modelo foi selecionado
- ğŸ”¨ **Cache miss**: Criando nova instÃ¢ncia do modelo
- â™»ï¸ **Cache hit**: Reutilizando modelo cacheado
- ğŸš€ **InvocaÃ§Ã£o**: Iniciando chamada
- âœ… **Sucesso**: LatÃªncia + tokens estimados
- âŒ **Falha**: Mensagem de erro
- ğŸ”„ **Retry**: Tentando fallback

**Exemplo de sessÃ£o de logs**:
```
[MultiModelChat] âœ… Initialized with model: gemini-2.5-flash

[MultiModelChat] ğŸ“‹ Task: ReadStructure
[MultiModelChat] ğŸ”¨ Creating new model instance: gemini-2.5-flash
[MultiModelChat] âœ… Model cached: gemini-2.5-flash
[MultiModelChat] ğŸš€ Invoking: gemini-2.5-flash
[MultiModelChat] [ReadStructure] âœ… Success: gemini-2.5-flash (1,123ms, ~5,234 tokens)

[MultiModelChat] ğŸ“‹ Task: Planning
[MultiModelChat] â™»ï¸  Using cached model: gemini-2.5-flash
[MultiModelChat] ğŸš€ Invoking: gemini-2.5-flash
[MultiModelChat] [Planning] âœ… Success: gemini-2.5-flash (1,456ms, ~7,890 tokens)
```

---

## ğŸ› ï¸ Ferramentas de DiagnÃ³stico

### 1. **Diagnostics Script**

```bash
cd c:\Users\HP\OneDrive\Desktop\Trabalho\Projetos\code-in
python -m agent.src.llm.diagnostics
```

**Output**:
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ¥ LLM Provider Diagnostics
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Step 1: Environment Configuration
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   OpenAI               âŒ Not configured
   Anthropic            âŒ Not configured
   Google AI            âœ… Configured
   Ollama               âœ… URL set: http://localhost:11434

ğŸ“‹ Step 2: Provider Health Checks
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Provider Health] âœ… google: Google AI provider configured
[Provider Health] âŒ openai: OpenAI API key not configured
[Provider Health] âŒ anthropic: Anthropic API key not configured
[Provider Health] âŒ ollama: Cannot connect to Ollama at http://localhost:11434

ğŸ“‹ Step 3: Available Models
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… 5 models available:
      â€¢ gemini-2.5-flash              (google)
      â€¢ gemini-2.5-pro                (google)
      â€¢ gemini-2.0-flash              (google)
      â€¢ gemini-3-flash                (google)
      â€¢ gemini-3-pro                  (google)

ğŸ“‹ Step 4: Default Model Selection
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Model Selection] Available models: ['gemini-2.5-flash', ...]
[Model Selection] âœ… Selected: gemini-2.5-flash
   Selected: gemini-2.5-flash
   âœ… Model gemini-2.5-flash is ready to use

ğŸ“‹ Step 5: Test Specific Models
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… gemini-2.5-flash              Model gemini-2.5-flash is ready to use
   âœ… gemini-2.0-flash              Model gemini-2.0-flash is ready to use
   âœ… gemini-2.5-pro                Model gemini-2.5-pro is ready to use

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š Summary
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   Healthy Providers:  1/4
   Available Models:   5
   Default Model:      gemini-2.5-flash

   â„¹ï¸  Some providers are not configured (this is normal)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 2. **Unit Tests**

```bash
python agent/tests/test_model_tracking.py
```

Testa toda a lÃ³gica sem dependÃªncias externas (100% mocked).

---

## ğŸ“¦ Arquivos Modificados

| Arquivo | MudanÃ§as |
|---------|----------|
| [`provider.py`](agent/src/llm/provider.py) | â€¢ `ModelUsageStats` e `ModelUsageTracker`<br>â€¢ `health_check()` em todos os providers<br>â€¢ `check_providers_health()`<br>â€¢ `validate_model_before_use()`<br>â€¢ `MultiModelChat` com retry + tracking<br>â€¢ Logging detalhado |
| [`__init__.py`](agent/src/llm/__init__.py) | â€¢ Exporta novas funÃ§Ãµes e classes |
| [`graph.py`](agent/src/graph/graph.py) | â€¢ Health check prÃ©-voo ao criar o grafo<br>â€¢ ValidaÃ§Ã£o do modelo selecionado |
| [`diagnostics.py`](agent/src/llm/diagnostics.py) | â€¢ Script de diagnÃ³stico completo (**NEW**) |
| [`test_model_tracking.py`](agent/tests/test_model_tracking.py) | â€¢ Testes unitÃ¡rios da lÃ³gica (**NEW**) |

---

## ğŸ¯ Antes vs Depois

### âŒ Antes

```
# SeleÃ§Ã£o silenciosa
chat = MultiModelChat()  # Qual modelo? NÃ£o se sabe

# Uso sem visibilidade
response = await chat.ainvoke(messages)  # Sucesso? LatÃªncia? Tokens? MistÃ©rio

# Falha sem contexto
# ERROR: quota exceeded (qual modelo? qual etapa?)

# EstatÃ­sticas: inexistentes
# Quanto custou a anÃ¡lise? Quantas chamadas fizemos? NÃ£o sabemos
```

### âœ… Depois

```
# SeleÃ§Ã£o explÃ­cita
[Model Selection] Available models: ['gemini-2.5-flash', ...]
[Model Selection] âœ… Selected: gemini-2.5-flash

# Uso rastreado
[MultiModelChat] ğŸ“‹ Task: Planning
[MultiModelChat] ğŸš€ Invoking: gemini-2.5-flash
[MultiModelChat] [Planning] âœ… Success: gemini-2.5-flash (1,234ms, ~3,456 tokens)

# Falha com contexto
[MultiModelChat] [Planning] âŒ Failed: gemini-2.5-pro - quota exceeded
[MultiModelChat] [Planning] ğŸ”„ Retrying with fallback: gemini-2.0-flash
[MultiModelChat] [Planning] âœ… Success: gemini-2.0-flash (890ms, ~2,541 tokens)

# EstatÃ­sticas completas
chat.print_usage_summary()
# ğŸ“Š Total: 15 invocations | 93.3% success | 42,156 tokens | 1,234ms avg
```

---

## ğŸš€ Como Usar

### CenÃ¡rio 1: Desenvolvimento Local

```bash
# 1. Verificar providers antes de comeÃ§ar
python -m agent.src.llm.diagnostics

# 2. Rodar o agente normalmente
python -m agent.src.main

# 3. No final, ver estatÃ­sticas de uso nos logs
# (jÃ¡ estÃ¡ integrado no MultiModelChat)
```

### CenÃ¡rio 2: Debugging de AnÃ¡lise

Se vocÃª quer saber **exatamente** qual modelo estÃ¡ sendo usado em cada etapa:

1. Rode o agente normalmente
2. Busque por logs `[MultiModelChat]` na saÃ­da
3. Veja task context, modelo usado, latÃªncia, tokens

**Exemplo de busca nos logs**:
```bash
# Ver quais modelos foram usados
grep "\[MultiModelChat\] ğŸš€" logs.txt

# Ver falhas
grep "\[MultiModelChat\] âŒ" logs.txt

# Ver retries
grep "\[MultiModelChat\] ğŸ”„" logs.txt
```

### CenÃ¡rio 3: OtimizaÃ§Ã£o de Custos

```python
# No final da sessÃ£o
chat.print_usage_summary()

# Analise:
# - Qual modelo foi mais usado?
# - Qual teve melhor success rate?
# - Quantos tokens consumimos no total?
# - LatÃªncia mÃ©dia estÃ¡ aceitÃ¡vel?
```

---

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# Google AI (recomendado - Ãºnico configurado por padrÃ£o)
GOOGLE_API_KEY=your_key_here

# OpenAI (opcional)
OPENAI_API_KEY=your_key_here

# Anthropic (opcional)
ANTHROPIC_API_KEY=your_key_here

# Ollama (opcional - servidor local)
OLLAMA_URL=http://localhost:11434
```

### Customizar Priority List

Edite [`provider.py`](agent/src/llm/provider.py):

```python
def get_default_model() -> str:
    """Get the default model based on what's available."""
    priority = [
        "gemini-2.5-flash",    # â† Mude a ordem aqui
        "gemini-2.0-flash",
        "gemini-2.5-pro",
        # ... adicione novos modelos
    ]
```

---

## ğŸ‰ BenefÃ­cios

1. **Visibilidade Total**: VocÃª sempre sabe qual modelo estÃ¡ sendo usado
2. **Debugging Facilitado**: Logs contextualizados por etapa do pipeline
3. **ResiliÃªncia**: Auto-retry automÃ¡tico em caso de falha
4. **OtimizaÃ§Ã£o**: EstatÃ­sticas de uso para otimizar custos
5. **ValidaÃ§Ã£o**: Health checks previnem erros de configuraÃ§Ã£o
6. **Manutenibilidade**: CÃ³digo mais claro e rastreÃ¡vel

---

## ğŸ“ PrÃ³ximos Passos (Fase 4 - Opcional)

- [ ] Integrar com sistema de billing real para custo exato
- [ ] Dashboard web para visualizar estatÃ­sticas em tempo real
- [ ] Alertas quando success rate cair abaixo de threshold
- [ ] Persistir estatÃ­sticas em banco de dados
- [ ] ComparaÃ§Ã£o de modelos (A/B testing automÃ¡tico)
- [ ] Rate limiting inteligente por modelo

---

## âœ… Status

**Implementado**: âœ… 100%  
**Testado**: âœ… Logic tests passando  
**Documentado**: âœ… Este arquivo  
**Pronto para produÃ§Ã£o**: âœ… Sim (precisa instalar dependÃªncias)

